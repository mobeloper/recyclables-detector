{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUxd2yg_Vz7e"
   },
   "source": [
    "# Auto-Generating COCO Annotations for Instance Segmentation using FastSAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_Rt24yRWBek"
   },
   "source": [
    "Instance segmentation requires high-quality annotations, but manual annotation is time-consuming and expensive. This notebook automates the annotation process by leveraging FastSAM, a lightweight and efficient segmentation model, to generate masks from images.\n",
    "\n",
    "The key steps in this pipeline include:\n",
    "\n",
    "\n",
    "\n",
    "1.   Mask Generation with FastSAM - Detects object masks quickly.\n",
    "2.   Post-processing - Reduces errors, removes false detections, and refines results.\n",
    "3.   COCO JSON Conversion - Converts masks into COCO format for training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QwU45_aWqVJ"
   },
   "source": [
    "## Import required libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rgua57f6X_mr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ipykernel_py3/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oysterable/delete/recyclables-detector/data_annotation\n",
      "Cloning into 'FastSAM'...\n",
      "remote: Enumerating objects: 1329, done.\u001b[K\n",
      "remote: Counting objects: 100% (401/401), done.\u001b[K\n",
      "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
      "remote: Total 1329 (delta 346), reused 308 (delta 308), pack-reused 928 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1329/1329), 72.56 MiB | 13.11 MiB/s, done.\n",
      "Resolving deltas: 100% (542/542), done.\n",
      "/Users/oysterable/delete/recyclables-detector/data_annotation/FastSAM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "\n",
    "%cd {HOME}\n",
    "\n",
    "# Clone the FastSAM repo and install the required libraries.\n",
    "!git clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n",
    "!pip -q install -r FastSAM/requirements.txt\n",
    "!pip -q install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "%cd {HOME}/FastSAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZGCFweSHWtVz"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndimage\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastsam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastSAM, FastSAMPrompt\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import skimage\n",
    "from fastsam import FastSAM, FastSAMPrompt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"DEVICE = {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "BDlLA2KRYXD_"
   },
   "outputs": [],
   "source": [
    "#@title Utils\n",
    "\n",
    "_PROPERTIES = (\n",
    "    \"major_axis_length\",\n",
    "    \"minor_axis_length\",\n",
    ")\n",
    "\n",
    "\n",
    "def masks_to_bool(masks: Union[np.ndarray, torch.Tensor]) -> np.ndarray:\n",
    "  \"\"\"Convert masks to boolean format.\n",
    "\n",
    "  Args:\n",
    "      masks: Input masks, either as a NumPy array or a PyTorch tensor.\n",
    "\n",
    "  Returns:\n",
    "      Boolean masks where values are converted to True/False.\n",
    "  \"\"\"\n",
    "  if type(masks) == np.ndarray:\n",
    "      return masks.astype(bool)\n",
    "  return masks.cpu().numpy().astype(bool)\n",
    "\n",
    "\n",
    "def plot_boolean_masks(masks: np.ndarray, masks_per_row: int = 5):\n",
    "    \"\"\"Plots boolean masks in a grid format with a fixed number of masks per row.\n",
    "\n",
    "    Args:\n",
    "        masks: Boolean masks.\n",
    "        masks_per_row: Number of masks to display per row.\n",
    "    \"\"\"\n",
    "    num_masks = masks.shape[0]  # Total number of masks\n",
    "    num_rows = (num_masks + masks_per_row - 1) // masks_per_row  # Compute required rows\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, masks_per_row, figsize=(masks_per_row * 3, num_rows * 3))\n",
    "\n",
    "    # Flatten axes array in case of a single row\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(len(axes)):\n",
    "        if i < num_masks:\n",
    "            axes[i].imshow(masks[i])  # Display mask\n",
    "            axes[i].axis(\"off\")  # Hide axis labels\n",
    "            axes[i].set_title(f\"Mask {i+1}\")  # Set title\n",
    "        else:\n",
    "            axes[i].axis(\"off\")  # Hide empty subplots\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def extract_properties(masks: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Extracts properties of masks and computes additional ratio features.\n",
    "\n",
    "    Args:\n",
    "        masks: Boolean masks.\n",
    "\n",
    "    Returns:\n",
    "        Extracted properties.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "\n",
    "    for mask in masks:\n",
    "      binary_mask = np.where(mask, 1, 0)\n",
    "      df = pd.DataFrame(\n",
    "        skimage.measure.regionprops_table(binary_mask, properties=_PROPERTIES)\n",
    "      )\n",
    "      dataframes.append(df)\n",
    "\n",
    "    features = pd.concat(dataframes, ignore_index=True)\n",
    "    features[\"axis_ratio\"] = features[\"major_axis_length\"] / features[\"minor_axis_length\"]\n",
    "    return features\n",
    "\n",
    "def _is_contained(mask1: np.ndarray, mask2: np.ndarray):\n",
    "  \"\"\"Check if mask1 is entirely contained within mask2.\n",
    "\n",
    "  Args:\n",
    "    mask1: The first mask.\n",
    "    mask2: The second mask.\n",
    "\n",
    "  Returns:\n",
    "    True if mask1 is entirely contained within mask2, False otherwise.\n",
    "  \"\"\"\n",
    "  return np.array_equal(np.logical_and(mask1, mask2), mask1)\n",
    "\n",
    "\n",
    "def _calculate_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:\n",
    "  \"\"\"Calculate the intersection over union (IoU) between two masks.\n",
    "\n",
    "  Args:\n",
    "    mask1: The first mask.\n",
    "    mask2: The second mask.\n",
    "\n",
    "  Returns:\n",
    "    The intersection over union (IoU) between the two masks.\n",
    "  \"\"\"\n",
    "  intersection = np.logical_and(mask1, mask2).sum()\n",
    "  union = np.logical_or(mask1, mask2).sum()\n",
    "  return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "def filter_masks(masks: np.ndarray, iou_threshold: float = 0.8) -> np.ndarray:\n",
    "  \"\"\"Filter the overlapping masks.\n",
    "\n",
    "  Filter the masks based on the intersection over union (IoU) and keep the\n",
    "  biggest masks if they are overlapping.\n",
    "\n",
    "  Args:\n",
    "    masks: The masks to filter.\n",
    "    iou_threshold: The threshold for the intersection over union (IoU) between\n",
    "      two masks.\n",
    "\n",
    "  Returns:\n",
    "    Unique masks.\n",
    "  \"\"\"\n",
    "  # Calculate the area for each mask\n",
    "  areas = np.array([np.sum(mask) for mask in masks])\n",
    "\n",
    "  # Sort the masks based on area in descending order\n",
    "  sorted_indices = np.argsort(areas)[::-1]\n",
    "  sorted_masks = masks[sorted_indices]\n",
    "\n",
    "  unique_masks = []\n",
    "\n",
    "  for i, mask in enumerate(sorted_masks):\n",
    "    keep = True\n",
    "    for j in range(i):\n",
    "      if _calculate_iou(mask, sorted_masks[j]) > iou_threshold or _is_contained(\n",
    "          mask, sorted_masks[j]\n",
    "      ):\n",
    "        keep = False\n",
    "        break\n",
    "    if keep:\n",
    "      unique_masks.append(mask)\n",
    "\n",
    "  return np.array(unique_masks)\n",
    "\n",
    "\n",
    "def keep_largest_component(masks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Keeps only the largest connected component in each binary mask.\n",
    "\n",
    "    Args:\n",
    "        masks: Binary masks.\n",
    "\n",
    "    Returns:\n",
    "        Boolean masks with only the largest component retained.\n",
    "    \"\"\"\n",
    "    largest_component_masks = []\n",
    "\n",
    "    for mask in masks:\n",
    "      mask = mask.astype(np.uint8)*255\n",
    "\n",
    "      # Find connected components\n",
    "      num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n",
    "          mask,\n",
    "          connectivity=8\n",
    "      )\n",
    "\n",
    "      # Find the largest component, excluding the background (label 0)\n",
    "      largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "\n",
    "      # Create a boolean mask for the largest connected component\n",
    "      largest_component_mask = labels == largest_label\n",
    "      largest_component_mask = ndimage.binary_fill_holes(largest_component_mask)\n",
    "      largest_component_masks.append(largest_component_mask)\n",
    "\n",
    "    return np.array(largest_component_masks)\n",
    "\n",
    "\n",
    "def create_coco_annotation_for_single_image(\n",
    "    binary_masks: np.ndarray,\n",
    "    labels: list[str],\n",
    "    image_name: str,\n",
    "    image_height: int,\n",
    "    image_width: int\n",
    "    ):\n",
    "    \"\"\"Creates a COCO annotation JSON.\n",
    "\n",
    "    Create an annotation file for instance segmentation from binary masks and\n",
    "    corresponding labels for a single image.\n",
    "\n",
    "    Args:\n",
    "      binary_masks: List of binary mask arrays corresponding to objects.\n",
    "      labels: List of labels corresponding to each mask in the image.\n",
    "      image_file: Image name.\n",
    "      image_height: Image height.\n",
    "      image_width: Image width.\n",
    "\n",
    "    Returns:\n",
    "      COCO-style annotation JSON as a Python dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # COCO structure template\n",
    "    coco_dataset = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []\n",
    "    }\n",
    "\n",
    "    # Add categories (assume labels are unique)\n",
    "    label_to_id = {label: idx + 1 for idx, label in enumerate(set(labels))}\n",
    "    for label, category_id in label_to_id.items():\n",
    "        coco_dataset[\"categories\"].append({\n",
    "            \"id\": category_id,\n",
    "            \"name\": label,\n",
    "            \"supercategory\": \"object\"\n",
    "        })\n",
    "\n",
    "    # Get the file name and path\n",
    "    file_name = os.path.basename(image_name)\n",
    "\n",
    "    # extract height and width\n",
    "    height, width = image_height, image_width\n",
    "\n",
    "    img_id = 1  # Since it's a single image, you can set the image ID to 1\n",
    "\n",
    "    # Add image information\n",
    "    coco_dataset[\"images\"].append({\n",
    "        \"id\": img_id,\n",
    "        \"width\": width,\n",
    "        \"height\": height,\n",
    "        \"file_name\": file_name\n",
    "    })\n",
    "\n",
    "    # Process each mask in the image\n",
    "    annotation_id = 1\n",
    "    for mask, label in zip(binary_masks, labels):\n",
    "        category_id = label_to_id[label]\n",
    "\n",
    "        # Find contours for the mask and flatten the contour points\n",
    "        contours, _ = cv2.findContours(\n",
    "            mask.astype(np.uint8),\n",
    "            cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        segmentation = []\n",
    "        for contour in contours:\n",
    "            contour = contour.flatten().tolist()  # Flatten the contour and convert it to a list\n",
    "            if len(contour) >= 6:  # A valid polygon needs at least 3 points (6 coordinates)\n",
    "                segmentation.append(contour)\n",
    "\n",
    "        # Calculate area and bounding box\n",
    "        area = int(np.sum(mask.astype(bool)))\n",
    "        bbox = cv2.boundingRect(mask.astype(np.uint8))\n",
    "        x, y, w, h = bbox\n",
    "\n",
    "        # Create annotation entry\n",
    "        coco_dataset[\"annotations\"].append({\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": category_id,\n",
    "            \"segmentation\": segmentation,  # Segmentation in polygon format\n",
    "            \"area\": area,\n",
    "            \"bbox\": [x, y, w, h],\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "\n",
    "        annotation_id += 1\n",
    "\n",
    "    for i in coco_dataset['annotations']:\n",
    "      i['segmentation'] = [max(i['segmentation'], key=len)]\n",
    "\n",
    "    # Return the COCO JSON object\n",
    "    return coco_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKgmXOkjaLwD"
   },
   "source": [
    "## Install FastSAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjynXNbJaNgX"
   },
   "outputs": [],
   "source": [
    "!mkdir weights\n",
    "!wget -P weights -q https://huggingface.co/spaces/An-619/FastSAM/resolve/main/weights/FastSAM.pt\n",
    "!ls -lh weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RraxgIKxbEul"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pn7JUcfabGYl"
   },
   "outputs": [],
   "source": [
    "FAST_SAM_CHECKPOINT_PATH = \"weights/FastSAM.pt\"\n",
    "fast_sam = FastSAM(FAST_SAM_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3tX3Ykh6fuL"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2VOFQTu7I5m"
   },
   "source": [
    "Fast SAM parameters:\n",
    "\n",
    "\n",
    "\n",
    "*   `retina_masks=True` determines whether the model uses retina masks for generating segmentation masks.\n",
    "*   `imgsz`=1024 sets the input image size to 1024x1024 pixels for processing by the model.\n",
    "*   `conf`=0.4 sets the minimum confidence threshold for object detection.\n",
    "*   `iou`=0.9 sets the minimum intersection over union threshold for non-maximum suppression to filter out duplicate detections.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDVzGCC6Z9IH"
   },
   "outputs": [],
   "source": [
    "# Import an image.\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/tensorflow/models/master/official/\"\n",
    "    \"projects/waste_identification_ml/pre_processing/config/sample_images/\"\n",
    "    \"sample_image_fastsam.jpeg\"\n",
    ")\n",
    "!curl -O {url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-swo_ZH96e0N"
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = \"sample_image_fastsam.jpeg\"\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aS6uJkeQ9nee"
   },
   "outputs": [],
   "source": [
    "results = fast_sam(\n",
    "    source=IMAGE_PATH,\n",
    "    device=DEVICE,\n",
    "    retina_masks=True,\n",
    "    imgsz=1024,\n",
    "    conf=0.5,\n",
    "    iou=0.1)\n",
    "prompt_process = FastSAMPrompt(IMAGE_PATH, results, device=DEVICE)\n",
    "masks = prompt_process.everything_prompt()\n",
    "\n",
    "if len(masks) == 0:\n",
    "  print(\"No masks detected\")\n",
    "masks = masks_to_bool(masks)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20o5ofDy_jOu"
   },
   "outputs": [],
   "source": [
    "plot_boolean_masks(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaRXiMnJPk5D"
   },
   "source": [
    "## Postprocessing masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmbUiEoCAd8Q"
   },
   "source": [
    "If you notice that Mask5 and Mask6 are the false positives which needs to be removed. We will use different techniques to get rid of such detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RJNPXo4Ax00"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(IMAGE_PATH)\n",
    "image_height, image_width = image.shape[:2]\n",
    "\n",
    "# Remove masks which are bigger than 30% of an image size and lower than 4000\n",
    "# pixels in area.\n",
    "HIGHER_THRESHOLD = 0.3 * image_height * image_width\n",
    "LOWER_THRESHOLD = 4000\n",
    "masks = np.array([mask for mask in masks if LOWER_THRESHOLD < np.sum(mask) < HIGHER_THRESHOLD])\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tW3cD0jwGdn5"
   },
   "outputs": [],
   "source": [
    "# Removes masks whose major to minor axis ratio is bigger than 5.\n",
    "features = extract_properties(masks)\n",
    "\n",
    "RATIO_THRESHOLD = 5\n",
    "masks = np.array([mask for mask,ratio in zip(masks, features[\"axis_ratio\"]) if ratio < RATIO_THRESHOLD])\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vHNTs6gLRUe"
   },
   "outputs": [],
   "source": [
    "# Keep the largest component masks if they are connected.\n",
    "mask = keep_largest_component(masks)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfN4u98sLPtL"
   },
   "outputs": [],
   "source": [
    "# Remove overlapped smaller masks and keep the biggest one using IoU.\n",
    "masks = filter_masks(masks)\n",
    "print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUYrwwIBsTYV"
   },
   "outputs": [],
   "source": [
    "plot_boolean_masks(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qY5oTDSRMpL"
   },
   "source": [
    "## Create COCO JSON annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PB8WBm6LRPc0"
   },
   "outputs": [],
   "source": [
    "# Get the class name of each corresponding mask.\n",
    "labels = ['non-bottle']*len(masks)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGdeaLLqRaYX"
   },
   "outputs": [],
   "source": [
    "# Create a COCO JSON format file.\n",
    "coco_json = create_coco_annotation_for_single_image(\n",
    "    masks,\n",
    "    labels,\n",
    "    os.path.basename(IMAGE_PATH),\n",
    "    image_height,\n",
    "    image_width\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDQZpy5qR4tL"
   },
   "outputs": [],
   "source": [
    "coco_json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miN0_A-mR7BJ"
   },
   "outputs": [],
   "source": [
    "coco_json['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHWcdW-PR83f"
   },
   "outputs": [],
   "source": [
    "coco_json['annotations'][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au0zisweR_up"
   },
   "outputs": [],
   "source": [
    "for i in range(len(masks)):\n",
    "  print(f\"id:{coco_json['annotations'][i]['id']}\\\n",
    "          image_id:{coco_json['annotations'][i]['image_id']}\\\n",
    "          category_id:{coco_json['annotations'][i]['category_id']}\\\n",
    "          area:{coco_json['annotations'][i]['area']}\\\n",
    "          bbox:{coco_json['annotations'][i]['bbox']}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNfiIcYWdzjSNDF95Sy/PJF",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1noxfBxYrdhHIPmWwHQkZ-YykM__MnqMJ",
     "timestamp": 1740508170028
    }
   ]
  },
  "kernelspec": {
   "display_name": "ipykernel_py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
